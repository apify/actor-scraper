{
    "title": "Puppeteer Scraper Input",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "title": "Start URLs",
            "type": "array",
            "description": "URLs to start with",
            "prefill": [
                { "url": "http://example.com" },
                { "url": "http://iana.org" }
            ],
            "editor": "requestListSources"
        },
        "useRequestQueue": {
            "title": "Use request queue",
            "type": "boolean",
            "description": "Request queue enables recursive crawling and the use of Pseudo-URLs and Link selector.",
            "default": true
        },
        "pseudoUrls": {
            "title": "Pseudo-URLs",
            "type": "array",
            "description": "Pseudo-URLs to match links in the page that you want to enqueue. Combine with Link selector to tell the crawler where to find links.",
            "editor": "pseudoUrls",
            "default": [],
            "prefill": [
                {
                    "purl": "https://www.iana.org/[((?!\\.pdf$).)*]"
                }
            ]
        },
        "linkSelector": {
            "title": "Link selector",
            "type": "string",
            "description": "CSS selector matching elements with 'href' attributes that should be enqueued. To enqueue urls from '&lt;div class=\"my-class\" href=...&gt;' tags, you would enter 'div.my-class'.",
            "editor": "textfield",
            "prefill": "a"
        },
        "pageFunction": {
            "title": "Page function",
            "type": "string",
            "description": "Function executed for each request",
            "prefill": "async ({ request, page, keyValueStore, log }) => {\n    const title = await page.title();\n    log.info(`Title of page with url ${request.url} is: ${title}`);\n    await keyValueStore.setValue(Math.random().toString(36), { title });\n    return { title }\n}",
            "editor": "javascript"
        },
        "proxyConfiguration": {
            "title": "Proxy configuration",
            "type": "object",
            "description": "Choose to use no proxy, Apify Proxy, or provide custom proxy URLs.",
            "prefill": { "useApifyProxy": false },
            "default": {},
            "editor": "proxy"
        },
        "debugLog": {
            "title": "Debug log",
            "type": "boolean",
            "description": "Debug messages will be included in the log. Use `context.log.debug('message')` to log your own debug messages.",
            "default": false,
            "groupCaption": "Options",
            "groupDescription": "Crawler settings"
        },
        "browserLog": {
            "title": "Browser log",
            "type": "boolean",
            "description": "Console messages from the Browser will be included in the log. This may result in the log being flooded by error messages, warnings and other messages of little value, especially with high concurrency.",
            "default": false
        },
        "downloadMedia": {
            "title": "Download media",
            "type": "boolean",
            "description": "Crawler will skip downloading media such as images, fonts, videos and sounds. This helps to speed up the crawl, but may break certain websites.",
            "default": false
        },
        "downloadCss": {
            "title": "Download CSS",
            "type": "boolean",
            "description": "Crawler will skip downloading CSS stylesheets. This helps to speed up the crawl, but may break certain websites.",
            "default": false
        },
        "ignoreSslErrors": {
            "title": "Ignore SSL errors",
            "type": "boolean",
            "description": "Crawler will ignore SSL certificate errors.",
            "default": false
        },
        "maxRequestRetries": {
            "title": "Max request retries",
            "type": "integer",
            "description": "Maximum number of times the request for the page will be retried in case of an error. Setting it to 0 means that the request will be attempted once and will not be retried if it fails.",
            "minimum": 0,
            "default": 3,
            "unit": "retries"
        },
        "maxPagesPerCrawl": {
            "title": "Max pages per crawl",
            "type": "integer",
            "description": "Maximum number of pages that the crawler will open. 0 means unlimited.",
            "minimum": 0,
            "default": 0,
            "unit": "pages"
        },
        "maxResultsPerCrawl": {
            "title": "Max result records",
            "type": "integer",
            "description": "Maximum number of results that will be saved to dataset. The crawler will terminate afterwards. 0 means unlimited.",
            "minimum": 0,
            "default": 0,
            "unit": "results"
        },
        "maxCrawlingDepth": {
            "title": "Max crawling depth",
            "type": "integer",
            "description": "Defines how many links away from the StartURLs will the crawler descend. 0 means unlimited.",
            "minimum": 0,
            "default": 0
        },
        "pageLoadTimeoutSecs": {
            "title": "Page load timeout",
            "type": "integer",
            "description": "Maximum time the crawler will allow a web page to load in seconds.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "secs"
        },
        "pageFunctionTimeoutSecs": {
            "title": "Page function timeout",
            "type": "integer",
            "description": "Maximum time the crawler will wait for the page function to execute in seconds.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "secs"
        },
        "customData": {
            "title": "Custom data",
            "type": "object",
            "description": "This object will be available on pageFunction's context as customData.",
            "default": {},
            "editor": "json"
        },
        "cookies": {
            "title": "Cookies",
            "type": "object",
            "description": "The provided cookies will be pre-set to all pages the scraper opens.",
            "default": {},
            "editor": "json"
        }
    },
    "required": ["startUrls", "pageFunction"]
}
