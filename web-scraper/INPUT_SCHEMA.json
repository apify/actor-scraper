{
    "title": "Web Scraper Input",
    "type": "object",
    "description": "Web Scraper loads <b>Start URLs</b> in the Chrome browser and executes <b>Page function</b> on each page to extract data from it. To follow links and scrape additional pages, set <b>Link selector</b> with <b>Pseudo-URLs</b> to specify which links to follow. Alternatively, you can manually enqueue new links in <b>Page function</b>. For details, see actor's <a href='https://apify.com/apify/web-scraper' target='_blank' rel='noopener'>README</a> or <a href='https://apify.com/docs/scraping/tutorial/introduction' target='_blank' rel='noopener'>Web scraping tutorial</a> in the Apify documentation.",
    "schemaVersion": 1,
    "properties": {
        "runMode": {
            "sectionCaption": "Basic configuration",
            "title": "Run mode",
            "type": "string",
            "description": "This property indicates the scraper's mode of operation. In DEVELOPMENT mode, the scraper ignores page timeouts, doesn't use sessionPool, opens pages one by one and enables debugging via Chrome DevTools.  Open the live view tab or the container URL to access the debugger. Further debugging options can be configured in the Advanced configuration section. PRODUCTION mode disables debugging and enables timeouts and concurrency. <br><br>For details, see <a href='https://apify.com/apify/web-scraper#run-mode' target='_blank' rel='noopener'>Run mode</a> in README.",
            "default": "PRODUCTION",
            "prefill": "DEVELOPMENT",
            "editor": "select",
            "enum": [
                "PRODUCTION",
                "DEVELOPMENT"
            ]
        },
        "startUrls": {
            "title": "Start URLs",
            "type": "array",
            "description": "A static list of URLs to scrape. <br><br>For details, see <a href='https://apify.com/apify/web-scraper#start-urls' target='_blank' rel='noopener'>Start URLs</a> in README.",
            "prefill": [
                { "url": "https://apify.com" }
            ],
            "editor": "requestListSources"
        },
        "pseudoUrls": {
            "title": "Pseudo-URLs",
            "type": "array",
            "description": "Specifies what kind of URLs found by <b>Link selector</b> should be added to the request queue. A pseudo-URL is a URL with regular expressions enclosed in <code>[]</code> brackets, e.g. <code>http://www.example.com/[.*]</code>. <br><br>If <b>Pseudo-URLs</b> are omitted, the actor enqueues all links matched by the <b>Link selector</b>.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#pseudo-urls' target='_blank' rel='noopener'>Pseudo-URLs</a> in README.",
            "editor": "pseudoUrls",
            "default": [],
            "prefill": [
                {
                    "purl": "https://apify.com[(/[\\w-]+)?]"
                }
            ]
        },
        "pageFunction": {
            "title": "Page function",
            "type": "string",
            "description": "JavaScript (ES6) function that is executed in the context of every page loaded in the Chrome browser. Use it to scrape data from the page, perform actions or add new URLs to the request queue.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#page-function' target='_blank' rel='noopener'>Page function</a> in README.",
            "prefill": "// The function accepts a single argument: the \"context\" object.\n// For a complete list of its properties and functions,\n// see https://apify.com/apify/web-scraper#page-function \nasync function pageFunction(context) {\n    // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n    // debugger; \n\n    // jQuery is handy for finding DOM elements and extracting data from them.\n    // To use it, make sure to enable the \"Inject jQuery\" option.\n    const $ = context.jQuery;\n    const pageTitle = $('title').first().text();\n\n    // Print some information to actor log\n    context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n\n    // Manually add a new page to the queue for scraping.\n    context.enqueueRequest({ url: 'http://www.example.com' });\n\n    // Return an object with the data extracted from the page.\n    // It will be stored to the resulting dataset.\n    return {\n        url: context.request.url,\n        pageTitle\n    };\n}",
            "editor": "javascript"
        },
        "proxyConfiguration": {
            "sectionCaption": "Proxy and browser configuration",
            "title": "Proxy must be FRENCH",
            "type": "object",
            "description": "Proxy servers to look as a FRENCH person, otherwise leboncoin will block you.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#proxy-configuration' target='_blank' rel='noopener'>Proxy configuration</a>. Should look like this : 'http://groups-RESIDENTIAL,country-FR:mmmdWt2t3Q83GNxTiuuyCZCghh@proxy.apify.com:8000' ",
            "prefill": { "useApifyProxy": false },
            "default": { "useApifyProxy": false },
            "editor": "proxy"
        },
        "maxPagesPerCrawl": {
            "title": "Max pages",
            "type": "integer",
            "description": "Maximum number of pages that the scraper will load. It will stop when this limit is reached (prevent excess platform usage for misconfigured scrapers).<br><br>If <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 0
        },
        "maxCrawlingDepth": {
            "title": "Max clicking depth",
            "type": "integer",
            "description": "How many links away from <b>Start URLs</b> the scraper will click on (=> avoid infinite crawling). <br><br>If <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 3
        }
    },
    "required": ["startUrls", "pageFunction", "proxyConfiguration"]
}
